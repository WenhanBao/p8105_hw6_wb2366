---
title: "p8105_hw6_wb2366"
auther: Wenhan Bao
Date: 2021-11-30
output: github_document
---

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(readr)
library(corrplot)
library(modelr)
library(mgcv)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## Problem1

**Load the dataset, clean and check for missing data**

```{r}
birthweight_df <- read_csv("birthweight.csv", show_col_types = F) %>%
  mutate(
    babysex = fct_infreq(as.factor(babysex)),
    frace = fct_infreq(as.factor(frace)),
    malform = fct_infreq(as.factor(malform)),
    mrace = fct_infreq(as.factor(mrace))
  )

apply(is.na(birthweight_df), 2, which) ## make sure that there is no missing value
```

**Propose a regression model for birthweight.(Based on the a data-driven model-building)** 

```{r}
birthweight_df %>%
  select(-babysex,-frace,-malform,-mrace,-pnumlbw,-pnumsga) %>%
  cor(method = "pearson") %>%
  corrplot(
    method = "color",
    type = "lower",
    tl.col = "Black",
    tl.srt = 45,
    diag = F,
    order = "AOE"
  )
```

```{r}
fit_bwt_le_he <- lm(bwt ~ blength + bhead, data = birthweight_df) 

fit_bwt_le_he %>%   
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```

```{r message=FALSE, warning=FALSE}
birthweight_df %>%
  modelr::add_residuals(fit_bwt_le_he) %>%
  modelr::add_predictions(fit_bwt_le_he) %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  geom_hline(yintercept = 0,
             col = "red",
             linetype = "dashed") +
  geom_smooth(method = "loess") +
  labs(title = "Residual vs Fitted Plot",
       x = "Fitted values",
       y = "Residuals") +
  theme(plot.title = element_text(hjust = 0.5))
```

**Describe the modeling process**

*Step1:* make a multiple Pearson's correlation coefficient matrix in which the result is closer to 1 or -1 means that there is a high correlation between those two variants.

*Step2:* Comparing the results, I notice that `blength`(baby’s length at birth) and `bhead`(baby’s head circumference at birth) have a potential correlation with `bwt`(baby's birth weight)

*Step3:* Build a linear regression model for baby's birth weight which may depend on baby’s length and baby’s head circumference at birth.

*Step4:* Here we see that linearity seems to hold reasonably well, as the blue line is close to the dashed line, especially for scale from 1500 to 4000 where most data falls in.


**Compare your model to two others:**

```{r}
set.seed(30)
cv_df <- crossv_mc(birthweight_df, 100) 

cv_df %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble)) %>%
  mutate(
    mod_blength_bhead = map(train, ~ lm(bwt ~ blength + bhead, data = .x)),
    mod_blength_gaweeks = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    mod_bheaed_blength_sex = map(
      train,
      ~ lm(bwt ~ babysex + bhead + blength + babysex * bhead + 
          babysex * blength + bhead * blength + 
          babysex * bhead * blength,data = .x))
  ) %>%
  mutate(
    rmse_mod_blength_bhead = map2_dbl(mod_blength_bhead, test, ~ rmse(model = .x, data = .y)),
    rmse_mod_blength_gaweeks = map2_dbl(mod_blength_gaweeks, test, ~ rmse(model = .x, data = .y)),
    rmse_mod_bheaed_blength_sex = map2_dbl(mod_bheaed_blength_sex, test, ~ rmse(model = .x, data = .y))) %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_mod_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, color = model)) + 
  geom_violin() + 
  labs(title = "RMSEs of Different Models",
       y = "RMSEs",
       x = "Model") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "none",
        panel.background = element_blank(),
        panel.grid = element_line(color = "azure1"))
```

**Conclusion:** Comparing the violin plot, we could figure out that for model 1 and 3 is more accurate while the RMSEs are much bigger in model 2 which means that the error rate of this linear regression model seems be high.

## Problem2

**Load the dataset.**

```{r message=FALSE, warning=FALSE}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

**Use 5000 bootstrap samples**  

```{r}
bootstrap_df <- 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    results_R = map(models, broom::glance),
    result_est = map(models, broom::tidy)) %>%
  unnest(results_R) %>% 
  select(.id, r.squared, result_est) %>% 
  unnest(result_est) %>% 
  select(1:4)
```

Distribution of $\hat{r}^2$

```{r}
r_squared_df <- bootstrap_df %>%
  select(.id, r.squared) %>%
  distinct()

plot_r_squared <- ggplot(r_squared_df,
                         aes(x = r.squared)) +
  geom_density(color = "chocolate1") +
  labs(y = "Density",
       x = "R.squared")  
```

Distribution of $log(\hat{\beta}_0 \ast \hat{\beta}_1)$

```{r}
log_df = bootstrap_df %>%
  select(.id, term, estimate) %>%
  group_by(.id) %>%
  mutate(
    estimate_beta0 = head(estimate, n = 1),
    estimate_beta1 = tail(estimate, n = 1)
  ) %>%
  select(.id, estimate_beta0, estimate_beta1) %>%
  distinct() %>%
  mutate(log_beta01 = log10(estimate_beta0 * estimate_beta1))

plot_log <- ggplot(log_df, 
                   aes(x = log_beta01)) +
  geom_density(color = "chartreuse") +
  labs(y = "Density",
       x = "log(β0 * β1)")

plot_r_squared + plot_log
```

**Describe the plot of two estimates:**
According to the density distribution of two plots, the estimates of $\hat{r}^2$ is almost 0.912 and of $log(\hat{\beta}_0 \ast \hat{\beta}_1)$ is nearly 0.875


**identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval**

```{r}
## 95% CI for r.squared
r_squared_df %>% 
  summarize(
    ci_lower = quantile(r.squared, 0.025), 
    ci_upper = quantile(r.squared, 0.975)) %>% 
  knitr::kable(caption = "95% CI for r.squared", digits = 3)

## 95% CI for log(β0 * β1)
log_df %>% 
  ungroup() %>% 
  summarize(
    ci_lower = quantile(log_beta01, 0.025), 
    ci_upper = quantile(log_beta01, 0.975)) %>% 
  knitr::kable(caption = "95% CI for log(β0 * β1)", digits = 3)

```




